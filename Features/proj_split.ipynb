{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/inespissarra/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/inespissarra/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import classification_report\n",
    "import sklearn_crfsuite\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "#                             Read the input database\n",
    "##################################################################################\n",
    "\n",
    "train = pd.read_csv('../train.txt', sep='\\t', header=None)\n",
    "train.columns = ['Class', 'Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after preprocessing\n"
     ]
    }
   ],
   "source": [
    "##################################################################################\n",
    "#                                Preprocessing\n",
    "##################################################################################\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "including = ['no', 'nor', 'not', 'but', 'against', 'only']\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def preprocess(text):\n",
    "    #tokenize\n",
    "    words = word_tokenize(text)\n",
    "    i = 0\n",
    "    text = \"\"\n",
    "    # transforming <word>n't in <word> not from words\n",
    "    while i < len(words):\n",
    "        # remove punctuation from words\n",
    "        words[i] = ''.join([char for char in words[i] if char not in string.punctuation])\n",
    "        # remove stopwords from words\n",
    "        if words[i] in stop and words[i] not in including:\n",
    "            words[i] = \"\"\n",
    "        #else:\n",
    "            # lemmatizing and Stemming from words\n",
    "            words[i] = stemmer.stem(lemmatizer.lemmatize(words[i]))\n",
    "        if text != \"\":\n",
    "            text = text + \" \"\n",
    "        if words[i]!=\"\":\n",
    "            text = text + words[i]\n",
    "        i = i+1\n",
    "    return text\n",
    "\n",
    "train['Text'] = train['Text'].apply(preprocess)\n",
    "\n",
    "print(\"after preprocessing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "#         Extracts features (and convert them to sklearn-crfsuite format)\n",
    "##################################################################################\n",
    "negation = [\"not\", \"no\", \"never\", \"neither\", \"nor\", \"none\", \"nobody\", \"nowhere\", \\\n",
    "            \"nothing\", \"hardly\", \"scarcely\", \"barely\", \"doesn't\", \"isn't\", \"wasn't\", \\\n",
    "                \"shouldn't\", \"wouldn't\", \"couldn't\", \"won't\", \"can't\", \"don't\", \"didn't\", \\\n",
    "                    \"aren't\", \"ain't\", \"without\"]\n",
    "sentiment = SentimentIntensityAnalyzer()\n",
    "def review2features(review):\n",
    "    tokens = nltk.word_tokenize(review)\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    for i in range(len(tokens)):\n",
    "        pol = sentiment.polarity_scores(tokens[i])\n",
    "        if ((i-1) >= 0 and tokens[i-1] in negation) and \\\n",
    "            (((i-2)>=0 and tokens[i-2]!=\"if\") or (i-2)<0):                                  # ver melhor, neg e pos\n",
    "            pol['compound'] = 0 - pol['compound']\n",
    "        if pol['compound'] > 0:\n",
    "            pos = pos + pol['compound']\n",
    "        else:\n",
    "            neg = neg - pol['compound']\n",
    "    pos = pos / len(tokens)\n",
    "    neg = neg / len(tokens)\n",
    "    polarity = pos - neg\n",
    "    features = [polarity]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after features\n"
     ]
    }
   ],
   "source": [
    "##################################################################################\n",
    "#               Creates different vectors (features, tags and tokens)\n",
    "##################################################################################\n",
    "X = [review2features(review) for review in train['Text']]\n",
    "\n",
    "y = train['Class']\n",
    "\n",
    "print(\"after features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after tfidf\n"
     ]
    }
   ],
   "source": [
    "##################################################################################\n",
    "#                                      TF-IDF\n",
    "##################################################################################\n",
    "\n",
    "tfidf = TfidfVectorizer(use_idf=True, ngram_range=(1, 3), sublinear_tf=True, max_features=20000)\n",
    "tfidf_matrix = tfidf.fit_transform(train['Text']).toarray()\n",
    "\n",
    "print(\"after tfidf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0.05153182 0.02363182 0.0279     0.11363636]\n",
      " [0.03891786 0.         0.03891786 0.03571429]\n",
      " [0.03290685 0.02538904 0.00751781 0.04109589]\n",
      " ...\n",
      " [0.02353889 0.01869222 0.00484667 0.06666667]\n",
      " [0.04083423 0.02494955 0.01588468 0.08108108]\n",
      " [0.09916429 0.         0.09916429 0.10714286]]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_matrix)\n",
    "print(np.array(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after combine\n"
     ]
    }
   ],
   "source": [
    "##################################################################################\n",
    "#                                     Combine\n",
    "##################################################################################\n",
    "\n",
    "combined_features = np.hstack((tfidf_matrix, np.array(X)))\n",
    "\n",
    "print(\"after combine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after split\n"
     ]
    }
   ],
   "source": [
    "##################################################################################\n",
    "#                                     Split\n",
    "##################################################################################\n",
    "\n",
    "X_train, \\\n",
    "    X_test, \\\n",
    "        y_train, \\\n",
    "            y_test \\\n",
    "                = train_test_split(combined_features, y, test_size=0.15, random_state=40)\n",
    "\n",
    "print(\"after split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TRUTHFULNEGATIVE' 'DECEPTIVENEGATIVE' 'TRUTHFULPOSITIVE'\n",
      " 'TRUTHFULNEGATIVE' 'DECEPTIVENEGATIVE' 'DECEPTIVEPOSITIVE'\n",
      " 'DECEPTIVEPOSITIVE' 'DECEPTIVEPOSITIVE' 'TRUTHFULPOSITIVE'\n",
      " 'DECEPTIVENEGATIVE' 'TRUTHFULPOSITIVE' 'TRUTHFULNEGATIVE'\n",
      " 'TRUTHFULPOSITIVE' 'TRUTHFULNEGATIVE' 'TRUTHFULPOSITIVE'\n",
      " 'DECEPTIVENEGATIVE' 'DECEPTIVENEGATIVE' 'DECEPTIVEPOSITIVE'\n",
      " 'TRUTHFULNEGATIVE' 'DECEPTIVEPOSITIVE' 'TRUTHFULPOSITIVE'\n",
      " 'TRUTHFULNEGATIVE' 'DECEPTIVENEGATIVE' 'TRUTHFULPOSITIVE'\n",
      " 'TRUTHFULNEGATIVE' 'DECEPTIVENEGATIVE' 'DECEPTIVENEGATIVE'\n",
      " 'DECEPTIVENEGATIVE' 'DECEPTIVENEGATIVE' 'DECEPTIVENEGATIVE'\n",
      " 'TRUTHFULPOSITIVE' 'TRUTHFULNEGATIVE' 'DECEPTIVENEGATIVE'\n",
      " 'DECEPTIVENEGATIVE' 'TRUTHFULPOSITIVE' 'TRUTHFULNEGATIVE'\n",
      " 'DECEPTIVENEGATIVE' 'DECEPTIVENEGATIVE' 'DECEPTIVENEGATIVE'\n",
      " 'TRUTHFULNEGATIVE' 'DECEPTIVEPOSITIVE' 'DECEPTIVEPOSITIVE'\n",
      " 'TRUTHFULNEGATIVE' 'TRUTHFULPOSITIVE' 'DECEPTIVEPOSITIVE'\n",
      " 'DECEPTIVEPOSITIVE' 'DECEPTIVEPOSITIVE' 'DECEPTIVEPOSITIVE'\n",
      " 'DECEPTIVEPOSITIVE' 'TRUTHFULPOSITIVE' 'DECEPTIVEPOSITIVE'\n",
      " 'DECEPTIVEPOSITIVE' 'TRUTHFULPOSITIVE' 'DECEPTIVENEGATIVE'\n",
      " 'TRUTHFULPOSITIVE' 'DECEPTIVEPOSITIVE' 'DECEPTIVEPOSITIVE'\n",
      " 'TRUTHFULPOSITIVE' 'DECEPTIVENEGATIVE' 'TRUTHFULNEGATIVE'\n",
      " 'DECEPTIVEPOSITIVE' 'TRUTHFULPOSITIVE' 'TRUTHFULNEGATIVE'\n",
      " 'TRUTHFULNEGATIVE' 'TRUTHFULPOSITIVE' 'TRUTHFULPOSITIVE'\n",
      " 'DECEPTIVEPOSITIVE' 'DECEPTIVEPOSITIVE' 'DECEPTIVENEGATIVE'\n",
      " 'DECEPTIVENEGATIVE' 'DECEPTIVEPOSITIVE' 'TRUTHFULPOSITIVE'\n",
      " 'DECEPTIVENEGATIVE' 'DECEPTIVEPOSITIVE' 'DECEPTIVEPOSITIVE'\n",
      " 'DECEPTIVEPOSITIVE' 'TRUTHFULPOSITIVE' 'DECEPTIVEPOSITIVE'\n",
      " 'DECEPTIVEPOSITIVE' 'DECEPTIVENEGATIVE' 'TRUTHFULPOSITIVE'\n",
      " 'DECEPTIVEPOSITIVE' 'DECEPTIVEPOSITIVE' 'TRUTHFULPOSITIVE'\n",
      " 'DECEPTIVENEGATIVE' 'DECEPTIVEPOSITIVE' 'TRUTHFULNEGATIVE'\n",
      " 'DECEPTIVEPOSITIVE' 'DECEPTIVENEGATIVE' 'DECEPTIVENEGATIVE'\n",
      " 'DECEPTIVENEGATIVE' 'DECEPTIVENEGATIVE' 'DECEPTIVENEGATIVE'\n",
      " 'DECEPTIVENEGATIVE' 'DECEPTIVEPOSITIVE' 'DECEPTIVEPOSITIVE'\n",
      " 'TRUTHFULPOSITIVE' 'DECEPTIVENEGATIVE' 'TRUTHFULNEGATIVE'\n",
      " 'TRUTHFULNEGATIVE' 'TRUTHFULNEGATIVE' 'TRUTHFULPOSITIVE'\n",
      " 'TRUTHFULPOSITIVE' 'TRUTHFULPOSITIVE' 'TRUTHFULNEGATIVE'\n",
      " 'TRUTHFULPOSITIVE' 'TRUTHFULPOSITIVE' 'TRUTHFULNEGATIVE'\n",
      " 'TRUTHFULPOSITIVE' 'TRUTHFULNEGATIVE' 'TRUTHFULNEGATIVE'\n",
      " 'DECEPTIVEPOSITIVE' 'TRUTHFULPOSITIVE' 'DECEPTIVEPOSITIVE'\n",
      " 'TRUTHFULNEGATIVE' 'DECEPTIVENEGATIVE' 'DECEPTIVEPOSITIVE'\n",
      " 'DECEPTIVEPOSITIVE' 'DECEPTIVEPOSITIVE' 'TRUTHFULPOSITIVE'\n",
      " 'TRUTHFULNEGATIVE' 'DECEPTIVEPOSITIVE' 'TRUTHFULNEGATIVE'\n",
      " 'DECEPTIVENEGATIVE' 'TRUTHFULPOSITIVE' 'DECEPTIVEPOSITIVE'\n",
      " 'DECEPTIVENEGATIVE' 'TRUTHFULNEGATIVE' 'DECEPTIVENEGATIVE'\n",
      " 'TRUTHFULPOSITIVE' 'DECEPTIVEPOSITIVE' 'DECEPTIVEPOSITIVE'\n",
      " 'TRUTHFULNEGATIVE' 'DECEPTIVEPOSITIVE' 'TRUTHFULPOSITIVE'\n",
      " 'DECEPTIVEPOSITIVE' 'TRUTHFULPOSITIVE' 'TRUTHFULNEGATIVE'\n",
      " 'DECEPTIVEPOSITIVE' 'DECEPTIVENEGATIVE' 'DECEPTIVEPOSITIVE'\n",
      " 'TRUTHFULPOSITIVE' 'TRUTHFULNEGATIVE' 'DECEPTIVEPOSITIVE'\n",
      " 'DECEPTIVENEGATIVE' 'TRUTHFULNEGATIVE' 'DECEPTIVENEGATIVE'\n",
      " 'DECEPTIVEPOSITIVE' 'DECEPTIVEPOSITIVE' 'TRUTHFULNEGATIVE'\n",
      " 'DECEPTIVEPOSITIVE' 'TRUTHFULNEGATIVE' 'TRUTHFULPOSITIVE'\n",
      " 'TRUTHFULNEGATIVE' 'DECEPTIVENEGATIVE' 'TRUTHFULPOSITIVE'\n",
      " 'TRUTHFULNEGATIVE' 'TRUTHFULNEGATIVE' 'TRUTHFULNEGATIVE'\n",
      " 'TRUTHFULPOSITIVE' 'TRUTHFULNEGATIVE' 'DECEPTIVENEGATIVE'\n",
      " 'DECEPTIVEPOSITIVE' 'TRUTHFULNEGATIVE' 'DECEPTIVENEGATIVE'\n",
      " 'TRUTHFULNEGATIVE' 'DECEPTIVENEGATIVE' 'TRUTHFULNEGATIVE'\n",
      " 'DECEPTIVEPOSITIVE' 'TRUTHFULNEGATIVE' 'TRUTHFULPOSITIVE'\n",
      " 'TRUTHFULPOSITIVE' 'TRUTHFULNEGATIVE' 'DECEPTIVEPOSITIVE'\n",
      " 'TRUTHFULNEGATIVE' 'DECEPTIVENEGATIVE' 'TRUTHFULNEGATIVE'\n",
      " 'TRUTHFULPOSITIVE' 'TRUTHFULNEGATIVE' 'DECEPTIVEPOSITIVE'\n",
      " 'TRUTHFULPOSITIVE' 'TRUTHFULNEGATIVE' 'DECEPTIVENEGATIVE'\n",
      " 'TRUTHFULPOSITIVE' 'DECEPTIVEPOSITIVE' 'DECEPTIVEPOSITIVE'\n",
      " 'DECEPTIVENEGATIVE' 'DECEPTIVEPOSITIVE' 'DECEPTIVENEGATIVE'\n",
      " 'TRUTHFULPOSITIVE' 'TRUTHFULPOSITIVE' 'DECEPTIVENEGATIVE'\n",
      " 'TRUTHFULNEGATIVE' 'DECEPTIVENEGATIVE' 'TRUTHFULPOSITIVE'\n",
      " 'TRUTHFULNEGATIVE' 'TRUTHFULPOSITIVE' 'TRUTHFULNEGATIVE'\n",
      " 'TRUTHFULPOSITIVE' 'TRUTHFULNEGATIVE' 'TRUTHFULNEGATIVE'\n",
      " 'DECEPTIVENEGATIVE' 'TRUTHFULPOSITIVE' 'TRUTHFULNEGATIVE'\n",
      " 'TRUTHFULNEGATIVE' 'DECEPTIVENEGATIVE' 'TRUTHFULNEGATIVE'\n",
      " 'TRUTHFULNEGATIVE' 'TRUTHFULNEGATIVE' 'DECEPTIVEPOSITIVE']\n"
     ]
    }
   ],
   "source": [
    "##################################################################################\n",
    "#                                     SVM\n",
    "##################################################################################\n",
    "clf = svm.SVC(kernel='linear')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "# ##################################################################################\n",
    "# #                                   Results\n",
    "# ##################################################################################\n",
    "\n",
    "# print(\"Classification Report:\")\n",
    "\n",
    "# print(classification_report(flat_y_test, flat_y_pred))\n",
    "\n",
    "# accuracy = accuracy_score(flat_y_test, flat_y_pred)\n",
    "\n",
    "# print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8523809523809524\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Accuracy: 0.8523809523809524 ++?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
